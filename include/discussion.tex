%discussion about the agility of the teams based on how they manage things
%discussion about the tools being close to the agile manifesto
%Dave Thomas argues about selling agility while we move away from agility (ruby grooves podcast - lister after 20th minute)
%http://msdn.microsoft.com/en-us/library/dd997578.aspx
%describe why it is difficult to measure. Companies adjust the methods to their needs. This is good, but the custom solutions are not measurable always Search for difficulties in agile or transitioning to agile. Difficulties in transition make people not adopting well the method and the practices.


%\section{Unkwown}
%
%The agile philosophy became widely known to the Software Engineering community through the agile manifesto \cite{beck2001agile} which was also listed in the Introduction of this document. Along with the four pillars of the manifesto lie 12 principles \cite{agile_principles} which try to provide to people a better understanding of what agile means. All of them combined, consist the essence of what it is to be Agile \cite{agile_alliance}.
%
%\begin{itemize}
%	\item Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.
%    \item Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.
%    \item Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.
%    \item Business people and developers must work together daily throughout the project.
%    \item Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.
%    \item The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.
%    \item Working software is the primary measure of progress.
%    \item Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.
%    \item Continuous attention to technical excellence and good design enhances agility.
%    \item Simplicity--the art of maximizing the amount of work not done--is essential.
%    \item The best architectures, requirements, and designs emerge from self-organizing teams.
%    \item At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behaviour accordingly.
%\end{itemize}
%
%Considering that the agile manifesto and its principles are the meaning of agility, then how well do the tools measuring agility succeed in doing so based on them?


\chapter{Discussion}
\label{ch:discussion}
\lettrine[lines=4, loversize=-0.1, lraise=0.1]{T}{his} chapter analyses the reasons for the results presented in Chapter~\ref{ch:results} and gives answers to the research questions (RQ) tabled in Chapter~\ref{ch:research_methodology}. Finally, it concludes with the presentation of validity threats and how these were mitigated while conducting this case study.

\section{Answers to Research Questions}

\subsection{RQ\#1 - Will \ac{PAM}, \ac{TAA} and \ac{OPS} yield the same results when measuring agility?}

The plots in Appendix~\ref{ch:correlation_plots} showed an unexpected and very interesting result. Not only do the tools lack a correlation, but they do not even have a monotonic relationship in comparison to each other for the agile practices covered (see Table~\ref{table:monotonic_relationships}), resulting in absence of convergent validity. This could indicate two things. The first one is that the results are random and the second one is that all three of the tools measure agility differently. 

As far as the aspect of the correlation results being random is concerned, there is a possibility that this is true. Maybe another approach on forming the data samples could provide different results, but the approach followed was evaluated as the most suitable at the beginning of the study.

On the other hand, the absence of monotonicity and the negative or extremely low correlations show that the questions used by the tools in order to cover an agile practice do it differently and that \ac{PAM}, \ac{TAA} and \ac{OPS} measure the agility of software development teams in their own unique way. Each of the tools was constructed and statistically validated during the development by its creators who had the agile concept in mind, but apparently they followed a different path in order to accomplish creating a tool for measuring agility. This is quite clear by looking into the different ways that each practice is covered (see Appendix~\ref{ch:mapping}), while many of the questions have a different perspective on measuring a practice although they focus on the same one. 

As it was explained in section~\ref{sec:direct_match_results}, almost all groups had different responses to the same questions. This could be due to two reasons. The first one is that the groups of direct match questions were not correctly formed and the second one is that the people have the tendency to judge a question differently. With regards to the aspect of the groups of direct match questions not being correctly formed, it seems that this has a low probability due to the fact that the questions were verified by the employees at the onset of the survey, as mentioned in section~\ref{subsubsec:direct_match_analysis}. On the other hand, according to \citet{Lacy}, survey respondents tend to give different answers to the same questions even weeks apart, something which is a common issue in surveys.

Coming to question \textbf{\textit{``Does convergent validity exist among the tools?"}}, we presented that convergent validity could not be established due to the low (if existing) correlations among the tools.

Coming to question \textbf{\textit{``Will the questions that are exactly the same among the tools yield the same results?"}}, we saw that a considerable amount of respondents' answers were different.

Coming to question \textbf{\textit{``What is the coverage of agile practices among tools?"}}, we saw that the tools did not cover the agile practices at the same level, neither in terms of questions plethora nor concerning the number of agile practices.

\ac{PAM} was validated with hundreds of subjects while \ac{TAA} has been used by many companies and \ac{OPS} has been used in case studies as well. Nevertheless, we conclude that \ac{PAM}, \ac{TAA} and \ac{OPS} do not yield the same results when measuring agility, although they should. The reasons for these unexpected phenomenon are explained in the following paragraphs.

\subsubsection{Few or no questions for measuring a practice}
Another reason for not being able to calculate the correlation of the tools is that they cover slightly or even not at all some of the practices. An example of this is the \textit{Smaller and Frequent Product Releases} practice. \ac{OPS} includes four questions, while on the other hand, \ac{PAM} and \ac{TAA} have a single question each. Furthermore, \textit{Appropriate Distribution of Expertise} is not covered at all by \ac{PAM}, while it is by the rest of the tools. In case the single question gets a low score, this will affect how effectively the tool will measure an agile practice. On the contrary, multiple questions can better cover the practice by examining more factors that affect it. Apart from measuring a practice more precisely, this also has the benefit that even if one question gets a low score, the rest of them are candidates for getting a higher one.

\subsubsection{The same practice is measured differently}
Something very interesting that came up during the data analysis was that although the tools cover the same practices, they do it in different ways, leading to different results. An example of this is the practice of \textit{Refactoring} (check figure ~\ref{fig:ref_plot}). \ac{PAM} checks whether there are enough unit tests and automated system tests to allow the safe code refactoring. In case the course unit/system tests are not developed by a team, the respondents will give low scores to the question, as the team members in company A did. Nevertheless, this does not mean that the team never refactors the software or that it does it with bad results. All teams in company A choose to refactor when it adds value to the system, but the level of unit tests is very low and they exist only for specific teams. On the other hand, \ac{TAA} and \ac{OPS} check how often the teams refactor among other factors.

\subsubsection{The same practice is measured in opposite questions}
\label{subsec:opposite_questions}
The \textit{Continuous Integration} practice has a unique paradox among \ac{TAA}, \ac{PAM} and \ac{OPS}. The first two tools include a question about the members of the team having synced to the latest code, while \ac{OPS} checks for the exact opposite. According to \citet{sventha_dissertation}, it is preferable for the teams not to share the same code in order to measure the practice. It is quite doubtful though how correct this question can be, since the \textit{Continuous Integration} requires frequent submits from the developers and thus the rest of the team will also have a local version of the code.

\subsubsection{Questions phrasing}
Although the tools might cover the same areas for each practice, the results could differ because of how a question is structured. An example of this is the \textit{Test Driven Development} practice. Both \ac{TAA} and \ac{PAM} ask about automated code coverage, while \ac{OPS} just asks about the existence of code coverage. Furthermore, \ac{TAA} focuses on 100\% automation, while \ac{PAM} doesnâ€™t. Thus, if a team has code coverage but it is not automated, then the score of the respective question should be low. In case of \ac{TAA}, if it is not fully automated, it should be even lower. It is evident that the abstraction level of a question has a great impact. The more specific it is, the more its answer will differ, resulting in possible low scores.
%\todo{so a problem with measuring agility is the right abstraction level. So we don't know how, or at what level, agility should be measured. Interesting. This means we have issues even for simple aspects as if they use TDD.}

\subsubsection{Survey answering}
According to \citet{Wagner_Zeglovits}, survey responses are affected mainly by two factors. One of them is the comprehension of a question and the other one is the judgement of a question. Although all respondents were free to ask about any question they did not understand, there is always the possibility that, for their own reasons, they preferred not to do it, a fact that might result in misunderstanding of a question's meaning. Moreover, the judgement of a person is extremely subjective which can lead to different approaches in giving an answer. Furthermore, \citet{Floyd_Fowler} argues that respondents can also answer to a question in a way that they will look good to the person reading the answers. 

\subsubsection{Better understanding of agile concepts}
In pre-post studies there is a possibility of the subjects becoming more aware of a problem in the second test due to the first test \cite{Campbell_Stanley}. Although the \textit{testing} threat as it is called does not directly apply here, the similar surveys on consecutive weeks could have enabled the respondents to take a deeper look into the agile concepts, resulting in better understanding them and consequently providing different answers to the surveys' questions. 

\subsubsection{How people perceive agility}
Although the concept of agility is not new, people do not seem to fully understand it, as \citet{Wang_Conboy} also mention. This is actually the reason for having so many tools in the field trying to measure how agile the teams are or the methodologies used. The teams implement agile methodologies differently and researchers create different measurement tools. There are numerous definitions about what agility is \cite{Kidd, Kara, Ramesh, agile_manufacturing}, and each of the tool creators adopt or adapt the tools to match their needs. Their only common basis is the agile manifesto \cite{beck2001agile} and its twelve principles \cite{agile_principles}, which are (and should be considered as) a compass for the agile practitioners. Nevertheless, they are not enough and this resulted in the saturation of the field. Moreover, \citet{conboy_fitzgerald} state that the agile manifesto principles do not provide practical understanding of the concept of agility. Consequently, all the reasons behind the current survey results are driven by the way in which tool creators and tool users perceive agility.

The questions in the surveys were all based on how their creators perceived the agile concept which is quite vague as \citet{tsourveloudis} have pointed out. As the reader has seen in previous chapters, \ac{PAM}, \ac{TAA} and \ac{OPS} focus on some common areas/practices, such as  \textit{Smaller and Frequent Product Releases} and \textit{High-Bandwidth Communication}, while many are different. None of the \citet{sventha_dissertation}, \citet{pam}, \citet{Leffingwell} claimed, of course, to have created the most complete measurement tool, but still, this leads to the oxymoron that the tools created by specialists to measure the agility of software development teams actually do it differently and without providing substantial solution to the problem. On the contrary, this leads to more confusion for the agile practitioners who are at their wits' end.

Considering that the researchers and specialists in the agile field perceive the concept of agility differently, it would be naive to say that the teams do not do the same. The answers to surveys are subjective and people reply to, depending on how they understand them. \citet{ambler} had commented the following ``I suspect that developers and management have different criteria for what it means to be agile", which shows that people do not see eye to eye. This is also corroborated by the fact that although a team works at the same room and follows the same processes for weeks, it is rather unlikely that its members will have the same understanding of what a retrospection or a releasing planning meeting means to them, a statement which is also supported by \citet{Williams_Microsoft}. This fact is not only mentioned by \citet{Dave_Thomas}, but also encouraged by stating that ``no two teams should be doing agile the same way".

\subsection{RQ\#2 - Can the tools be combined in a way that will provide a better approach in measuring agility?}

We have created an enhanced version of \ac{OPS} with the questions which existed only in \ac{PAM} and \ac{TAA}. Although the new tool has not been used and validated, we believe that the results should only be better considering the larger agile area covered by the additional questions. On the other hand though, more questions imply more effort and time from the respondents to spend, which could lead to opposites results than the desired ones. In Appendix~\ref{ch:ops_pam_taa}, one can see the enhanced version of \ac{OPS}.

\section{Threats to Validity}

\subsection{Construct Validity}
Construct validity mainly deals with obtaining the right method for the concept under study \cite{Wohlin}. We consider that the construct validity concerning the surveys given to the subjects was already handled by the creators of the tools which were used in this Master's Thesis. Our own construct validity lies in the convergent validity establishment in the chapters throughout this document. The small sample of subjects was the biggest threat in establishing convergent validity, making the results very specific to company A itself. A future work on this topic should be performed in another company to mitigate this threat. In order to avoid mono-method bias, some employees were asked to fill in the surveys first so as to detect any possible issues with them. All the subjects were promised to remain anonymous, resulting in mitigating the evaluation apprehension \cite{wohlin2012expse}. 

\subsection{Internal Validity}
Internal validity deals with the issues that may affect the casual relationship between treatment and results \cite{Wohlin}. The creators of \ac{PAM}, \ac{TAA} and \ac{OPS} have already tried to mitigate this when creating their tools. Yet, there are still some aspects of internal validity, such as selection bias maturation and testing effect. As far as maturation is concerned, this concerns the fatigue and boredom of the respondents. Although the surveys were small in size and did not require more than 15-20 minutes each, still the similar and possibly repetitive questions on the topic could cause fatigue and boredom to the subjects resulting in giving random answers to the survey questions. The mitigation for this threat was to separate the surveys and conduct them in three different periods. In addition, the respondents could stop the survey at any point and continue whenever they wanted. As far as the testing effect is concerned, this threat could not be mitigated. The testing effect threat applies in pre-post design studies only, but due to the same topic of the surveys, the subjects were to some extent more aware of what questions to expect in the second and third survey. Finally, selection could not be mitigated as well, since the case study focused on a specific company only.

\subsection{Conclusion Validity}
Conclusion validity concerns the possibility of reaching a wrong conclusion \cite{Wohlin}. Although the questions of the surveys have been carefully phrased by their creators, still there may be uncertainty about them. In order to mitigate this, for each survey a pilot one was conducted to spot any questions which would be difficult to understand. In addition, the participants could ask the author of this Master's Thesis for any questions they had concerning the survey questions. Finally, the statistical tests were run only for the data that satisfied the prerequisites, to mitigate the possibility of incorrect results. %Violated assumptions of the test statistics

\subsection{External Validity}
External validity deals with the ability to generalize the outcomes of the case study \cite{Wohlin}. This Master's Thesis was conducted in collaboration with one company and 30 subjects only. Consequently, it is hard for the outcomes to be generalisable. Nevertheless, we believe that any researcher replicating the case study in another organization with teams which follow the same agile practices as those used in company A and identified in Table~\ref{table:methodologyA_practices}, should have similar results.

\subsection{Reliability}
Reliability validity concerns the dependence of the data and the analysis on the specific researchers \cite{Wohlin}. To provide the ability to other researchers to conduct a similar study, the steps followed have been described and the reasons for the decisions made have been explained. Furthermore, all the data exist in digital format which can be provided to anyone who wants to review them. The presentation of the findings could be probably threatened by the author's experience. In order to mitigate this, the findings were discussed with a company A employee who did not participate in the case study.

%Maybe mention concurrent validity threat for the enhanced tool?
%http://dissertation.laerd.com/criterion-validity-concurrent-and-predictive-validity-p2.php

%Cronbach's alpha
%https://statistics.laerd.com/spss-tutorials/cronbachs-alpha-using-spss-statistics.php
%http://www.statisticshell.com/docs/reliability.pdf
%https://statistics.laerd.com/minitab-tutorials/cronbachs-alpha-using-minitab.php


%http://www.indiana.edu/~educy520/sec5982/week_9/520in_ex_validity.pdf
%http://en.wikipedia.org/wiki/Validity_%28statistics%29

%===
%Criterion?
%Concurrent? - http://en.wikipedia.org/wiki/Concurrent_validity

%===
%Content?
