\chapter{Research Methodology}
\label{ch:research_methodology}

%\newcommand{\mysubparagraph}[1]{\subparagraph{#1}\mbox{}\\}

\lettrine[lines=4, loversize=-0.1, lraise=0.1]{T}{his} chapter presents the case study conducted at company A. Its aim to check whether the different tools claiming to measure agility will yield the same results.

\section{Research Purpose}
The creators of \ac{PAM}, \ac{TAA} and \ac{OPS} state that their tool measures agility. However, the existence of not only the three of them, but also of the rest of the tools presented in Chapter~\ref{ch:related_work} implies that each one is considered by its creators as more appropriate than the others in measuring agility. The purpose of this study is to check whether these three tools will yield the same results.

\subsection{Research Questions}
\begin{enumerate}
	\item Will \ac{PAM}, \ac{TAA} and \ac{OPS} yield the same results?
	%by changing into "Roman" you will get capital letters
	\begin{enumerate}[label={\roman*)},align=left]
  		\item Does convergent validity exist among the tools?
  		\item Will the questions that are exactly the same among the tools yield the same results?
		\item What is the coverage of agile practices among tools?
	\end{enumerate}	
  	\item Can the tools be combined in a way that will provide a better approach in measuring agility?
\end{enumerate}

\subsection{Case Study}
Any effort to see if the selected agility measurement tools are valid in what they do, would require to apply them to real software developments teams. According to \citet{Runeson_Host} a case study is ``a suitable research methodology for software engineering research since it studies contemporary phenomena in their natural context". As a result, a case study was selected as the most suitable means for conducting the Master's Thesis.  

\section{Subject Selection}
Since all three agility measurement tools would be applied we wanted to find a company that would be willing and committed to spend time for as long as it was needed. For this reason,  company A was selected, since the author of this Master's Thesis is a member of the company's employees. In the following pages, we present information on the company's teams, products and the agile practices used.

\subsection{Company Description}
Company A is a United States company which activates in the Point Of Sales (POS) area. With the development of some new products, the company had a 400\% increase in the size of the development and quality assurance (QA) departments, which resulted in the need for better organizing the development and release processes. In addition, the increasing requests of new features in the company's systems require a more efficient way in delivering them to the customers and also maintaining the quality of the products.

\subsection{Methodology A}
In general, the company A does not follow a specific agile methodology, but rather a tailored mix of the most famous ones which suits the needs of each team. Methodology A, as we can name it, embraces the practices displayed in Table~\ref{table:methodologyA_practices} from the various agile methodologies, some of them to a larger and some of them to a smaller extent. The analysis made by \citet{koch2005agile} was used for identifying these methodologies. The identification of the practices was done by observing and understanding how the teams work. The results were verified by the agile coach of the teams.

\begin{table} [H]
\caption{Practices embraced by methodology A}
\begin{tabular}{| p{2cm} | p{13cm}|}
    \hline
     \textbf{Method} & \textbf{Practice} \\ \hline
     \textbf{XP}  & \begin{inparaenum} [a\upshape)]
     				\item Small Releases \item Simple design \item Refactoring \item Collective ownership \item Continuous integration \item 40-hour week \item Coding standards
					\end{inparaenum}      \\ \hline
     \textbf{FDD}  & \begin{inparaenum} [a\upshape)]  \item Developing by feature \item Feature teams \item Regular build schedule \item Inspections \item Configuration management
     				  \end{inparaenum}\\ \hline
     \textbf{Lean} & \begin{inparaenum} [a\upshape)] \item Empower the team \item Build Integrity In \item Amplify learning \item Eliminate waste
     				 \end{inparaenum} \\ \hline
\end{tabular}
\label{table:methodologyA_practices}
\end{table}

\subsection{Products}
Company A has developed a few products which belong to the following four areas: 
\begin{inparaenum} [a\upshape)]
\item desktop
\item mobile
\item cloud
\item platforms.
\end{inparaenum}
The names given correspond to the names of the teams that develop them.

\begin{itemize}
\item Product A - A series of three mobile applications which offer services to the stores or stores' customers.
\item Product B - A cloud application which offers services to product A and product D.
\item Product C - A platform used only by the company's employees. It supports services which are necessary for product D.
\item Product D - It is the main product of the company which is mostly used. The rest of the products were developed in order to support it and expand its functionalities.

\end{itemize}

\subsection{Teams}
There are four development teams, each for a product of the company. Some of the teams have mixed members of developers and testers. In the Tables~\ref{table:teamA}, \ref{table:teamB}, \ref{table:teamC}, \ref{table:teamD}, one can see the structure of the teams. \\

\begin{table}
  \RawFloats
 \begin{minipage}[b]{0.5\textwidth}
  \centering
    \caption{Team A - Profile} %Mobile
  \begin{tabular}{| p{3.3cm} | p{3cm}|}
    \hline
     \textbf{Team Size} & 7 \\ \hline
     \textbf{Roles}  & \begin{tabular}{@{}l@{}}Team Leader (1) \\ Developers (3) \\ Testers (3) \end{tabular} \\ \hline
     \textbf{Area} & Mobile \\ \hline
     \textbf{Tools Used}  & \begin{tabular}{@{}l@{}}Perforce \\ Titanium \end{tabular}  \\ \hline
     \textbf{Iteration Length}  & 2-3 weeks \\ \hline
  \end{tabular}
  \label{table:teamA}
 \end{minipage} %
%
 \begin{minipage}[b]{.5\textwidth}
  \centering
    \caption{Team B - Profile} %Marketing
  \begin{tabular}{| p{3.3cm} | p{3cm}|}
    \hline
     \textbf{Team Size} & 6 \\ \hline
     \textbf{Roles}  & \begin{tabular}{@{}l@{}}Team Leader (1) \\ Developers (5) \\ Testers (1) \end{tabular} \\ \hline
     \textbf{Area} & Java \\ \hline
     \textbf{Tools Used}  & \begin{tabular}{@{}l@{}}Perforce \\ Eclipse IDE \end{tabular} \\ \hline
     \textbf{Iteration Length}  & 2-3 weeks \\ \hline
  \end{tabular}
  \label{table:teamB}
  \end{minipage} %
  
  \vspace{10 mm}
%
 \begin{minipage}[b]{.5\textwidth}
  \centering
    \caption{Team C - Profile} %Info
  \begin{tabular}{| p{3.3cm} | p{3cm}|}
    \hline
     \textbf{Team Size} & 4 \\ \hline
     \textbf{Roles}  & \begin{tabular}{@{}l@{}}Team Leader (1) \\ Developers (2) \\ Testers (1) \end{tabular} \\ \hline
     \textbf{Area} & Java \\ \hline
     \textbf{Tools Used}  & \begin{tabular}{@{}l@{}}Perforce \\ Eclipse IDE \end{tabular}  \\ \hline
     \textbf{Iteration Length}  & 3-4 weeks \\ \hline
  \end{tabular}
  \label{table:teamC}
\end{minipage}%
%
 \begin{minipage}[b]{.5\textwidth}
  \centering
    \caption{Team D - Profile} %POS
  \begin{tabular}{| p{3.3cm} | p{3cm}|}
    \hline
     \textbf{Team Size} & 19 \\ \hline
     \textbf{Roles}  & \begin{tabular}{@{}l@{}}Team Leader (1) \\ Developers (10) \\ Testers (8) \end{tabular} \\ \hline
     \textbf{Area} & Java \\ \hline
     \textbf{Tools Used}  & \begin{tabular}{@{}l@{}}Perforce \\ Eclipse IDE \end{tabular} \\ \hline
     \textbf{Iteration Length}  & 2-4 weeks \\ \hline
  \end{tabular}
  \label{table:teamD}
  \end{minipage}
\end{table}




\section{Data Collection}
In order to collect the data, an online survey was considered to be the best option, since it could be easily answered by each subject. In addition, this would ensure no data loss. Google Drive\texttrademark \cite{google_drive} was selected to be the platform for collecting the data.

For each of the tools, four surveys were created per each team respectively. The data collection lasted about one month, while the surveys for each tool were conducted every ten days. First \ac{PAM} was sent, then \ac{TAA} and at last it was \ac{OPS}.

Two subjects were requested to answer to the surveys first, in order to detect if there were any questions which could cause confusion, but also to see how much time is needed to complete a survey. Once the issues pointed out by the two subjects were fixed, the surveys were sent to the rest of the company's employees.

The links to the surveys were sent to the subjects early in the morning via email, but they were asked to reply after lunch. The reasoning for this is that at the beginning of the day the employees need to perform tasks which are usually important and time consuming, while they must have a clear mind and attend meetings. On the contrary, after lunch, most of the employees try to relax by enjoying their coffee and discussing with each other. That time of the day was considered to be the best in order to ask them to spend 15-20 minutes and reply to the survey. The employees that belonged to more than one team were asked a couple of days later to take the other survey in order to verify that their answers match in both surveys. Every question of the surveys was mandatory. The participants were promised to remain anonymous.

As was mentioned in Chapter~\ref{ch:related_work}, \ac{PAM} focuses on the following agile practices:
\begin{inparaenum} [a\upshape)]
	\item Iteration Planning
	\item Iterative Development
	\item Continuous Integration And Testing
	\item Stand-Up Meetings
	\item Customer Access
	\item Customer Acceptance Tests
	\item Retrospectives
	\item Collocation.
\end{inparaenum}
From the aforementioned practices, methodology A does not support \textit{Stand-Up Meetings} and \textit{Retrospectives}, as a result, they were excluded from the surveys.

\ac{TAA} focuses on the following agile practices/areas:
\begin{inparaenum} [a\upshape)]
	\item Product Ownership
	\item Release Planning and Tracking
	\item Iteration Planning and Tracking
	\item Team
	\item Testing Practices
	\item Development Practices/Infrastructure.
\end{inparaenum}
From the above practices, methodology A does not support \textit{Product Ownership}, since it implies that the company A should implement Scrum, which it does not. Moreover, Scrum-oriented questions from the rest of the practices/areas were removed as well. 

Finally, \ac{OPS} focuses on the following strategies:
\begin{inparaenum} [a\upshape)]
	\item Iterative progression
	\item Incremental development
	\item Short delivery cycles
	\item Evolutionary requirements
	\item Continuous feedback
	\item Refactoring
	\item Test-first development
	\item Self-managing teams
	\item Continuous integration
	\item Constant velocity
	\item Minimal documentation
	\item High bandwidth communication
	\item Retrospection
	\item Client-driven iterations
	\item Distribution of expertise
	\item Configuration management
	\item Adherence to standards.
\end{inparaenum}

From the above practices, methodology F does not support \textit{Retrospection}. The company's policy for retrospective meetings is for them to be held only by two or three people, depending on how many persons were involved. According to the director of the Greek office, it is considered that meetings with the whole team do not have the desired results but rather the opposite, leading to more difficult communication and loss of time.
%\textit{Continuous feedback}, \textit{Constant velocity} .%(also check the analysis in section ~\ref{subsec:adequacy_analysis}

%About the questions of the agile practices/areas which were not included, they were all given a value of 1, the least possible value. 

As it was stated earlier, in subsection~\ref{subsec:ops}, \ac{OPS} agility measurements are based on three aspects: Adequacy, Capability and Effectiveness. Effectiveness measurement focuses on how well a team implements agile methodologies. Since the rest of the tools focus on the same thing, it was decided only to use the survey from Effectiveness and not to take into account the Adequacy and Capability.

For a clearer view on the questions contained in the surveys, one can take a look at Appendices~\ref{ch:effectiveness_hierarchy},~\ref{ch:pam} and~\ref{ch:team_agility_assessment}.

The surveys for \ac{PAM}, \ac{TAA} and \ac{OPS} were on a Likert scale 1-7 (never having done what is asked by the question to always doing what is asked by the question). From \ac{PAM}, only the \textit{Collocation} practice had its Likert scale 1-5 (team members being in different time zones to being in the same room), since its creators preferred it in this way. For the transformation of the results to a Likert scale 1-7 for the \textit{Collocation} practice, the formula~\eqref{eq:likert_transformation} \cite{likert_transformation} was used.  \begin{equation} \label{eq:likert_transformation} x_2 = (1.5 * x_1) - 0.5 \end{equation} 

%\subsubsection{Demographics} % Have to fix this. Better add a table
The employees who were asked to answer to the surveys where all members of the software development teams which consisted of software and quality assurance (QA) engineers. All of the participating employees have been in the company for over a year and most of them have more than five years of work experience in an agile environment. Employees who had been working for less than six months in the company were not asked to participate, since it was considered that they were not fully aware of the company's procedures or that they were not familiar enough with them. Although code review is practised, we avoided asking the code reviewers to take the same survey for the same team because it would not provide more value to the results. In addition, it might result in making them lose the interest in replying to the surveys. 

%\subsection{Collected Data}
Each participant replied to 176 questions in total. Initially, 34 surveys were expected to be filled in, but in the end 30 of them were filled in, since some employees chose not to participate.

\section{Data Preparation}
\label{subsec:data_preparation}
A preparation was necessary in order to conduct the data analysis. All three of the tools have different amount of questions and cover different practices. For this reason, we preferred to do a grouping of the questions based on the practices/areas to which they belong. In the following pages, we present a mapping between the questions used from the \ac{PAM} and \ac{TAA} tools with the practices from \ac{OPP} and the strategies from \ac{OPS}.

\subsubsection[\ac{TAA} Areas]{Team Agility Assessment - Areas}
Team Agility Assessment (\ac{TAA}) does not claim that it covers specific agile practices, but rather areas important for a team. It focuses on product ownership for Scrum teams but also on the release, iteration planning and tracking. The team factor plays a great role, as well as the development practices and the working environment. Automated testing is important here as well. Finally, it is worth mentioning that it is the only tool focusing to such an extent on the release planning. In Table~\ref{table:taa_practices} one can see \ac{TAA}'s areas.

\begin{table}
  \begin{tabular}{| p{5cm} p{5cm} |}
    \hline
    \multicolumn{2}{|c|}{\textbf{\ac{TAA} Areas}}  \\ \hline
     \begin{itemize} \item Product Ownership \item Release Planning and Tracking \item Iteration Planning and Tracking \end{itemize} &
     \begin{itemize}  \item Team \item Testing Practices \item Development Practices / Infrastructure \end{itemize}  \\ \hline
  \end{tabular}
  \captionof{table}{Areas covered by \ac{TAA}}
  \label{table:taa_practices}
\end{table}

\subsubsection[\ac{PAM} Practices]{Perceptive Agile Measurement - Practices}
The Perceptive Agile Measurement (\ac{PAM}) tool focuses on the iterations during software development but also on the stand-up meetings for the team members, their collocation and the retrospectives they have. The access to customers and their acceptance criteria have a high importance as well. Finally, the continuous integration and the automated unit testing are considered crucial in order to be agile. In Table~\ref{table:pam_practices} one can see \ac{PAM}'s practices.

\begin{table} [H]
  \begin{tabular}{| p{6cm} p{6cm} |}
    \hline
    \multicolumn{2}{|c|}{\textbf{\ac{PAM} Practices}}  \\ \hline
    	\begin{itemize} \item Iteration Planning \item Iterative Development \item Continuous Integration and Testing \item Collocation \end{itemize} &
     \begin{itemize} \item Stand-up Meetings \item Customer Access \item Customer Acceptance Tests \item Retrospectives \end{itemize}  \\ \hline
  \end{tabular}
  \captionof{table}{Agile practices covered by \ac{PAM}}
  \label{table:pam_practices}
\end{table}

\subsubsection[\ac{OPS} Practices]{Objectives, Principles, Strategies (OPS) - Practices}
Objectives, Principles, Strategies (\ac{OPS}) Framework is the successor of the Objectives, Principles, Practices (\ac{OPP}) Framework \cite{opp}. \ac{OPP} identified 27 practices as implementations of the principles which later on were transformed into 17 strategies. In Table~\ref{table:opp_practices} one can see \ac{OPP}'s practices. 

\begin{table}
\begin{tabular}{| p{7.5cm}  p{7.5cm} |}
	\hline
	\multicolumn{2}{|c|}{\textbf{\ac{OPP} Practices}}  \\ \hline
     	\begin{itemize}
     		\item Iterative and Incremental Development 
     		\item Continuous Feedback 
     		\item Evolutionary Requirements 
     		\item Smaller and Frequent Product Releases 
     		\item Customer/User Acceptance Testing 
     		\item Frequent Face-to-Face Communication
     		\item Refactoring 
     		\item Automated Test Builds
     		\item Software Configuration Management 
     		\item Test Driven Development
     		\item Iteration Progress Tracking and Reporting 
     		\item Code Ownership 
     		\item Retrospectives Meetings 
     		\item Just-in-Time Refinement of Features /Stories/Tasks 
     	\end{itemize} 
     	& \begin{itemize}
     	 	\item Appropriate Distribution of Expertise
  			\item Self-Organizing Teams
     		\item Client-Driven Iterations 
     		\item Product Backlog 
     		\item Agile Project Estimation 
     		\item Adherence to Coding Standards 
     		\item Physical Setup Reflecting Agile Philosophy
     		\item Daily Progress Tracking Meetings 
     		\item Minimal or Just Enough Documentation 
     		\item Minimal Big Requirements Up Front and Big Design Up Front 
     		\item Collocated Customers
     		\item Constant Velocity 
     		\item Pair Programming  
 		\end{itemize} 
     \\ \hline
\end{tabular}
\captionof{table}{Agile practices covered by \ac{OPP}}
\label{table:opp_practices}
\end{table}

\subsubsection[Tool Practices]{Practices Covered Among The Tools}
\label{subsubsec:practices_among_tools}

As can be clearly seen in Tables~\ref{table:taa_practices}, \ref{table:pam_practices} and \ref{table:opp_practices}, the \ac{OPP}, and as a consequence the \ac{OPS}, covers more agile practices than the other tools. In the next pages a mapping between \ac{OPP} and \ac{PAM} (see Table~\ref{table:opp_pam_practices}) and \ac{OPP} and \ac{TAA} (see Table~\ref{table:opp_taa_practices}) follows. \\

We have abstracted some of the \ac{OPP} practices to \ac{OPS} strategies in order to avoid repeating the mapping of the questions. These \ac{OPP} practices are: 
\begin{inparaenum} [a\upshape)]
	\item \textit{Frequent Face-to-Face Communication},
	\item \textit{Physical Setup Reflecting Agile Philosophy}, and
	\item \textit{Collocated Customers}
\end{inparaenum} and we have abstracted them to the \ac{OPS} strategy \textit{High-Bandwidth Communication}  \cite[p. 57]{sventha_dissertation}. In the same way, we have abstracted the \ac{OPP} \textit{Automated test builds} practice to the \ac{OPS} strategy \textit{Continuous Integration} \cite[p. 57]{sventha_dissertation}. \\

The connection between the practices and strategies is done based on the questions of each tool. The aforementioned connections are depicted with symbols. When a practice has more than one symbol, it is because it covers more practices from the other tool. \\

\begin{table}
\begin{tabular}{| p{6.8cm} | p{8cm} |}
	\hline
	\textbf{\ac{PAM}} & \textbf{\ac{OPP}/\ac{OPS}}  \\ \hline
		 \begin{itemize}[leftmargin=*, label=]
 			\item Iteration Planning \FourStar
 			\item Iterative Development \JackStarBold
 			\item Continuous Integration and Testing \AsteriskRoundedEnds 
 			\item Collocation \Asterisk 
 			\item Stand-up Meetings \EightStar
 			\item Customer Access \JackStar
 			\item Customer Acceptance Tests \AsteriskThin
 			\item Retrospectives \CrossMaltese
 		\end{itemize}
 		&
     	\begin{itemize}[leftmargin=*, label=]
     		\item Iteration Progress Tracking and Reporting \FourStar
     		\item Iterative and Incremental Development \FourStar ~\JackStarBold
     		\item Continuous Integration \AsteriskRoundedEnds
     		\item Software Configuration Management \AsteriskRoundedEnds
     		\item Test Driven Development \AsteriskRoundedEnds ~\AsteriskThin 
     		\item High-Bandwidth Communication \JackStar ~\Asterisk 
     		\item Daily Progress Tracking Meetings \EightStar
     		\item Client-Driven Iterations \JackStar ~\FourStar
     		\item Evolutionary Requirements \AsteriskThin
     		\item Customer/User Acceptance Testing \AsteriskThin
     		\item Retrospectives Meetings \CrossMaltese
     		\item Self-Organizing Teams \FourStar
 		\end{itemize} 
     \\ \hline
\end{tabular}
\caption{Relation of \ac{OPP}/\ac{OPS} and \ac{PAM} practices}
\label{table:opp_pam_practices}
\end{table}

\begin{table}
\begin{tabular}{| p{7cm} | p{7.8cm} |}
	\hline
	\textbf{\ac{TAA}} & \textbf{\ac{OPP}/\ac{OPS}}  \\ \hline
		\begin{itemize}[leftmargin=*, label=] 
     		\item Product Ownership \Asterisk 
     		\item Release Planning and Tracking \EightStar
     		\item Iteration Planning and Tracking \FourStar
     		\item Team \CrossMaltese
     		\item Testing Practices \AsteriskRoundedEnds
     		\item Development Practices/Infrastructure \JackStar	
 		\end{itemize} 
		&	
     	\begin{itemize}[leftmargin=*, label=]
     		\item Iterative and Incremental Development \Asterisk 
     	    \item Product Backlog \Asterisk 
     		\item Smaller and Frequent Product Releases \EightStar
     		\item Customer/User Acceptance Testing \FourStar ~\EightStar
     		\item Constant Velocity \FourStar	
     		\item Iteration Progress Tracking and Reporting \FourStar
     		\item Self-Organising Teams \CrossMaltese ~\EightStar ~\FourStar ~\JackStar 
     		\item Appropriate Distribution of Expertise \CrossMaltese
     		\item High-Bandwidth Communication \CrossMaltese 
     		\item Daily Progress Tracking Meetings \CrossMaltese
     		\item Retrospectives Meetings  \CrossMaltese ~\FourStar ~\EightStar 
     		\item Test Driven Development \AsteriskRoundedEnds
     		\item Refactoring \JackStar
     		\item Software Configuration Management \JackStar
     		\item Adherence to Coding Standards \JackStar
     		\item Pair Programming \JackStar
     		\item Continuous Integration \JackStar ~\AsteriskRoundedEnds
     	\end{itemize} 
     \\ \hline
\end{tabular}
\caption{Relation of \ac{OPP}/\ac{OPS} and \ac{TAA} practices/areas}
\label{table:opp_taa_practices}
\end{table}

\subsubsection{Mapping of questions among tools}
\label{subsubsec:mapping}

\ac{PAM} has its questions divided on the basis of agile practices, while on the other hand, \ac{TAA} has divided them based on areas considered important. As one can see from the tables above, while all practices/areas from \ac{PAM} and \ac{TAA} are mapped to \ac{OPP} and \ac{OPS}, not all of their questions are under \ac{OPP} practices or \ac{OPS} strategies. This can be explained due to the different perception/angle, that the creators of the tools have and what is considered important for an organization/team to be agile. The detailed mapping of the tools can be viewed in Appendix~\ref{ch:mapping}.


%\subsubsection{Tools Completeness Analysis}
%\label{subsec:tools_completeness_analysis}
%As far as research question ``How complete are the tools in measuring agility" is concerned, by viewing Tables~\ref{table:opp_pam_practices},~\ref{table:opp_taa_practices} and Appendix~\ref{ch:mapping}, one can clearly distinguish that \ac{OPP} and consequently \ac{OPS} is more complete than the others in measuring agility, covering all the areas of the \ac{PAM} and \ac{TAA} tools. Furthermore, as it can be seen in Table~\ref{table:questions_coverage}, the \ac{OPS} covers a high percent of questions from both tools directly and relevantly. The \ac{TAA} has a respective percent of non-relevant matches mostly due to \textit{Product Ownership} perspectives, which is not covered to such an extent from \ac{OPS}. This can be explained by the fact that \ac{OPS} covers basic  methodologies for developing software such as XP, FDD, Crystal, Lean \cite[p. 44]{sventha_dissertation}, whereas \textit{Product Ownership} refers explicitly to Scrum which is a method for managing product development \cite{koch2005agile}.
%
%\begin{table} [H]
%	\begin{tabular}{{| p{4cm} | p{4cm} | p{4cm} |}}
%		\hline
%		\multicolumn{3}{|c|}{\textbf{Questions Coverage}}  \\ \hline
%		\textbf{Match}  & \textbf{\ac{PAM}} & \textbf{\ac{TAA}}  \\ \hline		
%		Direct Match & 17/48 (35.4\%) & 25/68 (36.7\%) \\ \hline
%		Relevant Match & 31/48 (64.5\%) & 33/68 (48.5\%) \\ \hline
%		Non Relevant & 0/48 (0\%) & 10/68 (14.7\%) \\ \hline
%	\end{tabular}
%\caption{Questions Coverage from \ac{OPS}}
%\label{table:questions_coverage}
%\end{table} 

\section{Data Analysis}

The data gathered from the surveys were grouped on the basis of the practices covered by the \ac{OPP}, and as a consequence, the \ac{OPS}, as one can see in section~\ref{subsubsec:mapping}. From the 18 practices in total, four of them, \begin{inparaenum} [a\upshape)] \item Minimal or Just Enough Documentation \item Customer User Acceptance Testing \item Evolutionary Requirements \item Constant Velocity, \end{inparaenum} are covered only by one tool. The rest of the practices were covered by at least two.

\subsubsection{Convergent Validity Analysis}
\label{subsubsec:convergent_validity_analysis}
Since all the tools claim to be measuring agility and under the condition that convergent validity exists among them, then by definition they should yield the same results. The initial thought was to analyse the data for each team separately, but team A has such a small number of members that the results would be inadequate to work with. As a result, it was preferred to form the data sets for each practice based on the answers from all the teams. In Table~\ref{table:data_structure}, one can see the structure of the collected data.

\begin{table} [H]
	\caption{Collected Data Structure}
	\label{table:data_structure}
	\begin{tabular}{| c | c | c | c | c |} \hline
	\textbf{Practice} & \textbf{Participants} & \textbf{\ac{PAM}} & \textbf{\ac{TAA}} & \textbf{\ac{OPS}} \\ \hline
	\multirow{3}{*}{Practice1} & Participant1 & Score1 & Score1 & Score1 \\ \hhline{~----}
	& \vdots & \vdots & \vdots  & \vdots \\ \hhline{~----}
	& ParticipantN & ScoreN & ScoreN & ScoreN \\ \hline
	\end{tabular}
\end{table}

In similar studies \cite{jalali_angelis,  Delestras2013}, the correlation analysis was selected as the best way to check similar tools and this was followed here as well. Based on the analysis for \ac{PAM}, \ac{TAA} and \ac{OPS} which was  performed in section~\ref{subsec:data_preparation}, it is clear that \ac{OPS} covers more agile practices/areas. As a result, we decided to use the practices covered by each tool and see if they correlate with the same practices from the other two tools. The idea is based on the \textit{multitrait-multimethod matrix} presented by \citet{campbell1959}. The matrix is the most commonly used way for providing construct validity. Since the data are organised by practices and are gathered by different methodologies, we focus on examining the monotrait-heteromethod correlations only. In Appendix~\ref{ch:mapping}, one can see how the questions are grouped based on the \ac{OPS} practices. The data sets consist of the answers which were sorted by team, as stated previously, and have the same order in all practices (i.e. the nth answer in every practice is given by the same person).

For the mathematical calculations the RStudio\texttrademark \cite{rstudio} was selected, since it has a wide support from its community. 

In order to select from which correlation analysis method to choose, the data were checked in order to establish whether they had normal distribution or not. For this, the Shapiro-Wilk test was selected, as it appears to be the most powerful normality test according to a recent paper published by \citet{Razali}. In order for a distribution to be considered normal, the p-value must be greater than the alpha level, so as not to reject the null hypothesis and consider that the data are normally distributed. The chosen alpha level was 0.05, as it is the most common one.

Out of the 42 normality checks (three for each of the 14 practices), only 17 concluded that the data are normally distributed. The low level of normally distributed data gave a strong indication that the ``Spearman’s rank correlation coefficient", which is more adequate for non-parametric data, was more appropriate to use, rather than the ``Pearson product-moment correlation".

In order to use ``Spearman’s rank correlation coefficient", two prerequisites must be satisfied:
\begin{enumerate}
\item The two variables should be measured at the interval or ratio scale
\item There needs to be a monotonic relationship between the two variables
\end{enumerate}

The first prerequisite was covered thanks to the Likert scale. In order to check for the monotonicity, plots were drawn between the results of each tool for all 14 practices. The plots surprisingly showed that only 8 out of 42 were monotonic, which does not allow the use of `Spearman’s rank correlation coefficient" for the rest. Table~\ref{table:monotonic_relationships} summarizes the practices and the relationships which are monotonic and `Spearman’s rank correlation coefficient" can be used. The monotonic relationships are marked in green while the non-monotonic in red.

%New environment to save space in the monotonic's table
\newenvironment{mntnc_itemize} 
{ 
	\begin{itemize}
    \setlength{\itemsep}{0pt}  
    \setlength{\parskip}{0pt}  
}
{ \end{itemize}  }

\begin{table}
	{\renewcommand{\arraystretch}{0.2}%
	\begin{tabular}{ | p{4cm} | p{4cm} | p{4cm} |} \hline
		\begin{tabular} { p{4cm} } 
			Adherence to Standards \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize} 
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Appropriate Distribution of Expertise \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Client Driven Iterations \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{green}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} \\ \hline
		
		\begin{tabular} { p{4cm} } 
			Continuous Feedback \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{green}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Continuous Integration \\
			\begin{mntnc_itemize}  \item \colorbox{green}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			High Bandwidth Communication \\
			\begin{mntnc_itemize}  \item \colorbox{green}{\ac{PAM}-\ac{TAA}} \item \colorbox{green}{\ac{PAM}-\ac{OPS}} \item \colorbox{green}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} \\ \hline
		
		\begin{tabular} { p{4cm} } 
			Iteration Progress Tracking \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Iterative and Incremental Development \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{green}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Product Backlog \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} \\ \hline
		
		\begin{tabular} { p{4cm} } 
			Refactoring \\
			\begin{mntnc_itemize}  \item \colorbox{green}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Self Organizing Teams \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Smaller and Frequent Releases \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} \\ \hline
		
		\begin{tabular} { p{4cm} } 
			Software Configuration Management \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Test Drive Development \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} & 
		\begin{tabular} { p{4cm} } 
			Constant Velocity \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} \\ \hline
		
		\begin{tabular} { p{4cm} } 
			Minimal or Just Enough Documentation \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Customer User Acceptance Testing \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} &
		\begin{tabular} { p{4cm} }
			Evolutionary Requirements \\
			\begin{mntnc_itemize}  \item \colorbox{red}{\ac{PAM}-\ac{TAA}} \item \colorbox{red}{\ac{PAM}-\ac{OPS}} \item \colorbox{red}{\ac{TAA}-\ac{OPS}} \end{mntnc_itemize}
		\end{tabular} \\ \hline
	\end{tabular}}
	\caption{Monotonic Relationships}
	\label{table:monotonic_relationships}	
\end{table}

\subsubsection{Direct Match Questions Analysis}
\label{subsubsec:direct_match_analysis}
At the beginning, we had to find which questions are the same among the tools. 
In order to achieve this, the mapping described in section~\ref{subsubsec:mapping} was used. Afterwards, the questions were checked one by one to identify the ones which had the same meaning. When we finalized the groups of questions which were the same, we requested from the same employees who were taking the pilot surveys to verify if they believed the groups were correctly formed. Their answer was affirmative, so we continued by checking if the answers of the subjects were the same. In order to depict the results, we used heatmaps generated by RStudio\texttrademark \cite{rstudio}. Heatmaps were considered as ideal since the similarities or differences in the colours would make it easier for the readers to see the results. 

In Table~\ref{table:direct_match_results}, one can see the number of direct matches among the tools. Surprisingly, \ac{OPS}-\ac{TAA} have 20 questions with the same meaning, while \ac{OPS}-\ac{PAM} and \ac{TAA}-\ac{PAM} only 4 and 3 respectively. The direct match list of questions can be viewed in Appendix~\ref{ch:direct_match_questions}. 

\begin{table} [H]
	\begin{tabular}{{| c | c | c |}}
		\hline
		\multicolumn{3}{|c|}{\textbf{\large{Direct Match Questions Results}}}  \\ \hline
		\textbf{OPS - TAA} & \textbf{OPS - PAM} & \textbf{TAA - PAM} \\ \hline
		20 & 4 & 3 \\ \hline
	\end{tabular}
\caption{Direct Match Questions Among Tools - Results}
\label{table:direct_match_results}
\end{table} 

In order to see if the results from the matches are correct, we decided to use a statistical method. For the mathematical calculations the RStudio\texttrademark \cite{rstudio} was used again. In order to decide statistical which method to choose, the data were checked in order to establish whether they had normal distribution or not. For this, the Shapiro-Wilk test was used. The chosen alpha level was 0.05.

Out of the 35 normality checks (two for each group and three for one group) only 2 concluded that the data are normally distributed. Since the samples are also independent (they do not affect one another) there is a strong indication that the ``Mann–Whitney U test" is the appropriate one.

In order to use ``Mann–Whitney U test", three prerequisites must be satisfied

\begin{enumerate}
	\item Independent groups
	\item Non-normal distribution of the data
	\item Homoscedasticity 
\end{enumerate} 

Since the first two prerequisites were already satisfied we checked that the samples are indeed homogenous.

For the group \hyperref[G3]{G3} (``Smaller And Frequent Product Releases") we used the ``Kruskal–Wallis one-way analysis of variance" method which is the respective statistical method for more than two groups.

The hypothesis in both cases was:
\begin{itemize}[label={}]
	\item $H_0$: \textit{There is no difference between the groups of the same questions}
	\item $H_1$: \textit{There is a difference between the groups of the same questions}
\end{itemize}



\subsubsection{Tools' Agile Practices Coverage Analysis}
\label{subsubsec:coverage_analysis}
During the last years, Laurie Williams conducted two case studies \cite{Williams_Microsoft, laurie_williams} concerning the agile practices which are highly praised by practitioners. We picked out the most popular of them in order see at what extent the tools cover practices that are essential for software development in the industry. For identifying which practices from the case studies apply to the three tools, we used the analysis made in Appendix~\ref{ch:mapping}. In the relevant analysis, all questions were separated based on the agile practice to which they belonged.

\section{Chapter Summary}
In this chapter we presented the research questions, the research methodology followed and the data collection and analysis performed for the needs of this case study. The answers in the surveys given by the respondents were grouped in agile practices and based on them we tried to establish convergent validity. Furthermore, we analysed if the same questions which we identified across the three tools were given the same answers from the respondents. Finally, we checked to what extent do the tools cover agile practices. The above actions were taken in order to see if the \ac{PAM}, \ac{TAA} and \ac{OPS} will yield the same results.

%The questions among the tools will be matched based on whether they are covered directly, relevantly, or not at all. Direct match will be considered the one when a question from a tool has the same meaning with a question from another tool. %Relevant match will be considered the one where a question of a tool does not exist in \ac{OPS}, but its practice does exist in \ac{OPS}. Non-relevant match will be considered the one where a question cannot be matched at all in \ac{OPS}.